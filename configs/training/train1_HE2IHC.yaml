image_finetune: false
output_dir: ./TrainResult
checkpoint_output_dir: ./Checkpoint
pretrained_model_path: ./weights/dvst_pretrained
pretrained_vae_path: ./weights/dvst_pretrained

motion_module: ""

model_type: dvst_modules

valid_seed: 42
recycle_seed: 200

# stage1 trained
checkpoint_path: ""
# resume
resume_step_offset: 0
appearance_time_step: 0

unet_additional_kwargs:
  cross_attention_dim: null
  unet_use_cross_frame_attention: false
  unet_use_temporal_attention: false
  use_motion_module: false
  motion_module_resolutions:
  - 1
  - 2
  - 4
  - 8
  motion_module_mid_block: false
  motion_module_decoder_only: false
  motion_module_type: Vanilla
  motion_module_kwargs:
    num_attention_heads: 8
    num_transformer_block: 1
    attention_block_types:
    - Temporal_Self
    - Temporal_Self
    temporal_position_encoding: true
    temporal_position_encoding_max_len: 32
    temporal_attention_dim_div: 1


noise_scheduler_kwargs:
  num_train_timesteps: 1000
  beta_start:          0.00085
  beta_end:            0.012
  beta_schedule:       "scaled_linear"
  steps_offset:        1
  clip_sample:         false
  rescale_betas_zero_snr: true

infer_noise_scheduler_kwargs:
  algorithm_type: "dpmsolver++"
  beta_end: 0.02
  beta_schedule: "linear"
  beta_start: 0.0001
  dynamic_thresholding_ratio: 0.995
  euler_at_final: false
  lower_order_final: true
  num_train_timesteps: 1000
  prediction_type: "epsilon"
  sample_max_value: 1.0
  solver_order: 2
  solver_type: "midpoint"
  steps_offset: 0
  thresholding: false
  timestep_spacing: "linspace"
  trained_betas: null
  use_karras_sigmas: false
  use_lu_lambdas: false
  variance_type: null

train_data:
  data_dir_pre: ./data/HE2IHC
  resolution:         [512, 512]
  img_resolution:     [1024, 1024]
  video_length:       4
  group_size:         1
  pose_dir:           HE
  target_dir:         IHC
  source_from_self:   false
  blur_ref_image:     true


validation_data:
  source_image:
    - ./assets/HE2IHC/IHC/patch_30400_58624.png
    - ./assets/HE2IHC/IHC/patch_30400_60672.png
    - ./assets/HE2IHC/IHC/patch_32448_48384.png
  pose_image:
    - ./assets/HE2IHC/HE/patch_42048_26240.png
    - ./assets/HE2IHC/HE/patch_42048_26240.png
    - ./assets/HE2IHC/HE/patch_42048_26240.png
    
  target_image:
    - ./assets/HE2IHC/IHC/patch_42048_26240.png
    - ./assets/HE2IHC/IHC/patch_42048_26240.png
    - ./assets/HE2IHC/IHC/patch_42048_26240.png

  num_inference_steps: 25
  guidance_scale: [1.5, 1.5] # HE, CLIP. Adjust it will bring different results.
  sample_size: [512, 512]
  frame_stride: 1
  video_length: 8
  alpha: 1.0
  block_size: 32

context:
  context_frames: 32
  context_stride: 1
  context_overlap: 0

trainable_modules:
  - "unet"
  # - "attn_adapter"

gradient_accumulation_steps: 2 # total_batch_size = batch_size * gradient_accumulation_steps

learning_rate:    1.e-5
# batch_size = video_length * train_batch_size
train_batch_size: 8
num_workers: 8

max_train_epoch:      100
checkpointing_steps:  2500

validation_steps:       2500
validation_steps_tuple: []

mixed_precision_training: true
enable_xformers_memory_efficient_attention: true
gradient_checkpointing: false
is_debug: False
